{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64c2f193-4efa-45b2-bba5-7ea1d9c6dfb4",
   "metadata": {},
   "source": [
    "## Web Scraping Task â€“ CodeVeda Internship\n",
    "\n",
    "For my first task during the CodeVeda internship, I was assigned to scrape data from the web using **BeautifulSoup**. I chose to scrape table data about the **world's highest revenue companies**.\n",
    "\n",
    "### Challenges Encountered\n",
    "\n",
    "While working on this task, I ran into a few challenges:\n",
    "\n",
    "1. **Malformed Table Rows**  \n",
    "   After successfully scraping the table headers, I noticed that the `<tbody>` section of the table was not structured correctly. It contained only `<td>` elements without any enclosing `<tr>` rows. This made it technically invalid and difficult to work with.  \n",
    "   To solve this, I grouped the `<td>` elements based on the number of headers so that each group formed a complete row.\n",
    "\n",
    "2. **Advertisements Embedded in Table**  \n",
    "   The table included breakpoints with advertisement text that were not part of the actual data. These ad blocks were placed inside the table structure, using `<td>` tags, which made them appear as if they were part of the data.  \n",
    "   I resolved this by filtering out elements that did not fall between valid opening and closing `<td>` tags.\n",
    "\n",
    "3. **Pagination Handling**  \n",
    "   The table data was paginated, with a \"Next\" button linking to subsequent pages. I handled this by identifying the pagination link and programmatically fetching and parsing all pages to gather the full dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2307287a-7b29-4650-a05a-2e6734e23c26",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.13.3' requires the ipykernel package.\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages."
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635ff6cf-8319-495b-99c0-d045769decd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://companiesmarketcap.com/largest-companies-by-revenue/\"\n",
    "page = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(page.text, 'html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd70197c-f449-41a3-bb67-2d654ed83ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = soup.find('table', class_='default-table')\n",
    "# print(table.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60e281b-c064-401d-a464-445d1a33b181",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_table_titles = table.find_all(\"th\")\n",
    "table_titles = [title.text for title in table_titles]\n",
    "table_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb508867-3185-4c06-9d4d-b715e3e698fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "tbody = soup.find('tbody')\n",
    "tds = tbody.find_all('td', recursive=False)\n",
    "clean_tds = [td for td in tds if td.name == \"td\" and td.contents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b24e85e-cc4e-4f73-b925-0924fa7ea6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_per_row = len(table_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b17ea2a-7b77-4687-8d6c-f429a840f057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group tds into rows\n",
    "rows = [\n",
    "    clean_tds[i:i + columns_per_row]\n",
    "    for i in range(0, len(clean_tds), columns_per_row)\n",
    "]\n",
    "\n",
    "# rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632dbace-6351-4443-9f5d-b879d16a2f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert rows to plain text\n",
    "data = []\n",
    "for row in rows:\n",
    "    data.append([cell.get_text(strip=True) for cell in row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2d1360-fc67-4fe1-9dd4-8cd9724de5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data, columns=table_titles)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa7ac06-30ad-41ed-b6d9-c8eb05bc7f13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7008225e-2716-4f11-8950-27818577d56b",
   "metadata": {},
   "source": [
    "### Handle paginated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51732577-95fa-4348-8ac1-d128efdf5fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://companiesmarketcap.com\"\n",
    "start_url = \"/largest-companies-by-revenue/\"\n",
    "\n",
    "all_data = []\n",
    "next_page = start_url\n",
    "\n",
    "# count = 1\n",
    "\n",
    "while next_page:\n",
    "    print(f\"Fetching: {next_page}\")\n",
    "    response = requests.get(base_url + next_page)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    # Extract your <td> data here\n",
    "    tds = tbody.find_all('td', recursive=False)\n",
    "    clean_tds = [td for td in tds if td.name == \"td\" and td.contents]\n",
    "    all_data.extend([td.get_text(strip=True) for td in clean_tds])\n",
    "\n",
    "    # Find the \"Next\" button\n",
    "    next_link = soup.find(\"a\", class_=\"page-link\", string=lambda text: text and \"Next\" in text)\n",
    "\n",
    "    if next_link and next_link.get(\"href\"):\n",
    "        next_page = next_link[\"href\"]\n",
    "    else:\n",
    "        next_page = None  # no more pages\n",
    "    # count = count + 1 \n",
    "\n",
    "# At the end, `all_data` contains all scraped text data from all <td>s\n",
    "print(f\"Scraped {len(all_data)} td items.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c90150-9d08-4606-8ac5-233217044a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = [\n",
    "    all_data[i:i + columns_per_row]\n",
    "    for i in range(0, len(all_data), columns_per_row)\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecffdf76-9da4-440e-916b-d88f2148f7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data=rows, columns=table_titles)\n",
    "df.sample(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd223e91-41ae-497c-a39e-6896147e281b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV\n",
    "df.to_csv('companies.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04496849-c214-4271-badb-fb8de6679e8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f493cd-1ab5-400d-b0e8-6275e6cc150d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7c9e47-3034-42a0-91a9-3e145d6f2f83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
